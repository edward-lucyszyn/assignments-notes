\section{Exercise VI: Tree-based Methods and Ensembles}

(a) We opt to split the decision tree to maximize information gain, i.e., minimize entropy. \\ \\
For entropy calculations, according to \cite{1}, we have:

\begin{center}
\begin{tikzpicture}
  % Nodes
    \node[draw, ellipse, align=center] (A) at (2,2.5) {Studied};
    \node[draw, align=center] (B) at (0,0) {Passed: 2 \\ Failed: 3};
    \node[draw, align=center] (C) at (4,0) {Passed: 2 \\ Failed: 1};
  
  % Draw lines
  \draw (A) -- node[midway, above left] {No} (B);
  \draw (A) -- node[midway, above right] {Yes} (C);
\end{tikzpicture}
\end{center}

\[
    I(\texttt{Studied = Yes}) = -\frac{2}{3}\log_2(\frac{2}{3}) -\frac{1}{3}\log_2(\frac{1}{3}) = 0.918 ;
\]
\[
   I(\texttt{Studied = No}) = -\frac{2}{5}\log_2(\frac{2}{5}) -\frac{3}{5}\log_2(\frac{3}{5}) = 0.971 ;
\]
\[
\boxed{
   I(\texttt{Studied}) = \frac{3}{8}I(\texttt{Studied = Yes}) + \frac{5}{8}I(\texttt{Studied = No}) = 0.951.}
\]

For the entropy of "Slept":

\begin{center}
\begin{tikzpicture}
  % Nodes
    \node[draw, ellipse, align=center] (A) at (2,2.5) {Slept};
    \node[draw, align=center] (B) at (0,0) {Passed: 4 \\ Failed: 1};
    \node[draw, align=center] (C) at (4,0) {Passed: 0 \\ Failed: 3};
  
  % Draw lines
  \draw (A) -- node[midway, above left] {No} (B);
  \draw (A) -- node[midway, above right] {Yes} (C);
\end{tikzpicture}
\end{center}

\[
   I(\texttt{Slept = Yes}) = -\frac{0}{3}\log_2(\frac{0}{3}) -\frac{3}{3}\log_2(\frac{3}{3}) = 0 ;
\]
\[
   I(\texttt{Slept = No}) = -\frac{1}{5}\log_2(\frac{1}{5}) -\frac{4}{5}\log_2(\frac{4}{5}) = 0.722 ;
\]
\[
\boxed{
   I(\texttt{Slept}) = \frac{3}{8}I(\texttt{Slept = Yes}) + \frac{5}{8}I(\texttt{Slept = No}) = 0.451.}
\]

For the entropy of "Submit Assignments":

\begin{center}
\begin{tikzpicture}
  % Nodes
    \node[draw, ellipse, align=center] (A) at (2,3) {Submit Assignments};
    \node[draw, align=center] (B) at (0,0) {Passed: 1 \\ Failed: 3};
    \node[draw, align=center] (C) at (4,0) {Passed: 3 \\ Failed: 1};
  
  % Draw lines
  \draw (A) -- node[midway, above left] {No} (B);
  \draw (A) -- node[midway, above right] {Yes} (C);
\end{tikzpicture}
\end{center}

\[
   I(\texttt{Submit Assignments = Yes}) = -\frac{1}{4}\log_2(\frac{1}{4}) -\frac{3}{4}\log_2(\frac{3}{4}) = 0.811 ;
\]
\[
   I(\texttt{Submit Assignments = No}) = -\frac{1}{4}\log_2(\frac{1}{4}) -\frac{3}{4}\log_2(\frac{3}{4}) = 0.811 ;
\]

\begin{equation*}
    \addtolength{\fboxsep}{5pt}
    \boxed{
    \begin{gathered}
         I(\texttt{Submit Assignments}) = \frac{4}{8}I(\texttt{Submit Assignments = Yes}) \\
         + \frac{4}{8}I(\texttt{Submit Assignments = No}) = 0.811.
    \end{gathered}
    }
\end{equation*}

Thus, the feature to split on that minimizes the entropy is "Slept". The information gain is the following:

\[
    \boxed{
    \Delta_{\text{info}}(\texttt{Slept}) = I(\texttt{Parent}) - I(\texttt{Slept}) = 1 - 0,451 = 0,549.
    }
\]

(b) At each node we choose to split on the feature that maximizes the information gain. We obtain the following decision tree.

\begin{center}
\begin{tikzpicture}
  % Nodes
    \node[draw, ellipse, align=center] (A) at (-2, 0) {Slept};
    \node[draw, align=center] (B) at (-4.25, -2.5) {Failed\\ (Id: 1, 2, 3)};
    \node[draw, ellipse, align=center] (C) at (0.25,-2.5) {Submit Assignments};
    \node[draw, align=center] (D) at (-2,-5) {Passed \\ (Id: 4, 5, 7)};
    \node[draw, ellipse, align=center] (E) at (2.5,-5) {Studied};
    \node[draw, align=center] (F) at (4.75,-7.5) {Failed\\ (Id: 8)};
    \node[draw, align=center] (G) at (0.25,-7.5) {Passed \\ (Id: 6)};
    
  
  % Draw lines
  \draw (A) -- node[midway, above left] {Yes} (B);
  \draw (A) -- node[midway, above right] {No} (C);
  \draw (C) -- node[midway, above left] {Yes} (D);
  \draw (C) -- node[midway, above right] {No} (E);
  \draw (E) -- node[midway, above right] {No} (F);
  \draw (E) -- node[midway, above left] {Yes} (G);
\end{tikzpicture}
\end{center}

(c) If we construct a one-split tree with the split identified in part (a), we have the following decision tree.

\begin{center}
\begin{tikzpicture}
  % Nodes
    \node[draw, ellipse, align=center] (A) at (2,2.5) {Slept};
    \node[draw, align=center] (B) at (0,0) {Failed};
    \node[draw, align=center] (C) at (4,0) {Passed};
  
  % Draw lines
  \draw (A) -- node[midway, above left] {Yes} (B);
  \draw (A) -- node[midway, above right] {No} (C);
\end{tikzpicture}
\end{center}

In the first iteration, AdaBoost assigns a weight of $D_1(i) = \frac{1}{8}$ to each student $i \in \llbracket 1, 8 \rrbracket$. With this first classifier, there is only one error. Thus the error is: $$\epsilon_1 = \frac{1}{8}.$$
Then, according to the algorithm described in \cite{2} we compute the gradient step:
\[
    \alpha_1 = \frac{1}{2}\ln(\frac{1 - \epsilon_1}{\epsilon_1}) = \frac{1}{2}\ln(7).
\]
We then update the weights of the examples:
\[
\forall i \in \llbracket 1, 8 \rrbracket, \quad D_{2}(i) = \frac{D_{1}(i) e^{-\alpha_{1}y_{i}h_{1}(x_{i})}}{Z_{1}}
\]
where \(Z_{1}\) is the normalization factor equal to \(\displaystyle 2\sqrt{\epsilon_{1}(1-\epsilon_{1})}=\frac{1}{4}\sqrt{7}\).

At the end,

\[
    \boxed{\forall i \in \llbracket 1, 7 \rrbracket, \quad
    D_2(i) = \frac{1}{14}; \quad
    D_2(8) = \frac{1}{2}.
    }
\]